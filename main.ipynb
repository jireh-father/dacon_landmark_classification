{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "- ubuntu 16.04\n",
    "- python 3.6.5\n",
    "- cuda 10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 준비 \n",
    "- data.zip 다운로드\n",
    "- 현재 프로젝트 root경로를 기준으로 아래처럼 경로가 세팅되도록 압축을 풀어주세요.\n",
    "\n",
    "- 학습 이미지 경로 샘플\n",
    " - ./data/public/train/경기도/만안교/만안교_001.JPG\n",
    "\n",
    "- 테스트 이미지 경로 샘플\n",
    " - ./data/public/test/0/0b9jdr0e39.JPG\n",
    "\n",
    "- train.csv\n",
    " - ./data/public/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 총 15개의 모델 학습 \n",
    "- efficientnet b0~b7\n",
    "- fishnet150, fishnet201\n",
    "- resnext\n",
    "- efficientnet b3, b3, b4, b7 with all training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습1\n",
    "- model: efficientnet-b0\n",
    "- arcface loss\n",
    "- multiple pooling concat(GeM,MAC,SPoC)\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, horizontal flip, shift, scale\n",
    "- input size: 216(h)x384(w)\n",
    "\n",
    "## 2개의 GPU로 학습시킬 경우\n",
    "CUDA_VISIBLE_DEVICE=**0,1** python main.py **--data_parallel** ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb0\n",
      "log_dir work_landmark/log_efb0\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b0\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 33\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 96\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/33, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/33,step:49/827,2020/11/21 02:04:25] total_elapsed: 0:00:26.803006, batch_elapsed: 0.456604, 50 steps_elapsed: 23.551927\n",
      "loss: 7.930936, acc: 0.041667, lr: 0.001000, multi_batch_loss: 8.127976, multi_batch_acc: 0.031458, multi_batch_f1: 0.011616\n",
      "[train-epoch:01/33,step:99/827,2020/11/21 02:04:54] total_elapsed: 0:00:56.014256, batch_elapsed: 0.432997, 50 steps_elapsed: 25.341226\n",
      "loss: 6.354610, acc: 0.291667, lr: 0.001000, multi_batch_loss: 7.049923, multi_batch_acc: 0.130000, multi_batch_f1: 0.074069\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 185, in train_epoch\n",
      "    outputs = model(inputs, labels)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/arc_face_net.py\", line 87, in forward\n",
      "    return self.extract_feat(x, label)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/arc_face_net.py\", line 92, in extract_feat\n",
      "    x = self.backbone.extract_features(x)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/efficientnet_pytorch/model.py\", line 182, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/efficientnet_pytorch/model.py\", line 83, in forward\n",
      "    x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/efficientnet_pytorch/utils.py\", line 144, in forward\n",
      "    x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=96 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b0 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=33 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb0 \\\n",
    "--log_dir=work_landmark/log_efb0 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습2\n",
    "- model: efficientnet-b1\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, horizontal flip\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb1\n",
      "log_dir work_landmark/log_efb1\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name efficientnet-b1\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 25\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 56\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /home/ubuntu/.cache/torch/checkpoints/efficientnet-b1-f1951068.pth\n",
      "100.0%\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/25, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/25,step:49/1417,2020/11/21 02:09:45] total_elapsed: 0:00:22.683480, batch_elapsed: 0.397736, 50 steps_elapsed: 21.005457\n",
      "loss: 6.784391, acc: 0.035714, lr: 0.001000, multi_batch_loss: 6.890787, multi_batch_acc: 0.020714, multi_batch_f1: 0.007991\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 37, in _projection\n",
      "    cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 30, in _cosine_similarity\n",
      "    return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=56 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=efficientnet-b1 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=25 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb1 \\\n",
    "--log_dir=work_landmark/log_efb1 \\\n",
    "--transform_func_name=get_train_transforms_simple_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습3\n",
    "- model: efficientnet-b2\n",
    "- arcface loss\n",
    "- multiple pooling concat(GeM,MAC,SPoC)\n",
    "- label smoothing: 0.1\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb0\n",
      "log_dir work_landmark/log_efb0\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b0\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 33\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 96\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/33, LR: 0.001000\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=56 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b2 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=43 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb2 \\\n",
    "--log_dir=work_landmark/log_efb2 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습4\n",
    "- model: efficientnet-b3\n",
    "- arcface loss\n",
    "- multiple pooling concat(GeM,MAC,SPoC)\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb3\n",
      "log_dir work_landmark/log_efb3\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b3\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 33\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 52\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /home/ubuntu/.cache/torch/checkpoints/efficientnet-b3-5fb5a3c3.pth\n",
      "100.0%\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/33, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/33,step:49/1526,2020/11/21 02:16:23] total_elapsed: 0:00:28.203575, batch_elapsed: 0.502544, 50 steps_elapsed: 26.523921\n",
      "loss: 8.291828, acc: 0.000000, lr: 0.001000, multi_batch_loss: 8.406569, multi_batch_acc: 0.009231, multi_batch_f1: 0.003011\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 248, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=52 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b3 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=41 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb3 \\\n",
    "--log_dir=work_landmark/log_efb3 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습5\n",
    "- model: efficientnet-b4\n",
    "- arcface loss\n",
    "- multiple pooling concat(GeM,MAC,SPoC)\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb4\n",
      "log_dir work_landmark/log_efb4\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b4\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 33\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 32\n",
      "val_batch_size 64\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /home/ubuntu/.cache/torch/checkpoints/efficientnet-b4-6ed6700e.pth\n",
      "100.0%\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/33, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/33,step:49/2479,2020/11/21 02:18:32] total_elapsed: 0:00:25.417067, batch_elapsed: 0.461826, 50 steps_elapsed: 24.216140\n",
      "loss: 8.066511, acc: 0.000000, lr: 0.001000, multi_batch_loss: 8.470769, multi_batch_acc: 0.006875, multi_batch_f1: 0.001127\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 39, in _projection\n",
      "    if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=32 \\\n",
    "--val_batch_size=64 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b4 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=32 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb4 \\\n",
    "--log_dir=work_landmark/log_efb4 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습6\n",
    "- model: efficientnet-b5\n",
    "- arcface loss\n",
    "- multiple pooling concat(GeM,MAC,SPoC)\n",
    "- label smoothing: 0.1\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb5\n",
      "log_dir work_landmark/log_efb5\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b5\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 45\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 56\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth\" to /home/ubuntu/.cache/torch/checkpoints/efficientnet-b5-b6417697.pth\n",
      "100.0%\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/45, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/45,step:49/1417,2020/11/21 02:22:26] total_elapsed: 0:00:49.450611, batch_elapsed: 0.915322, 50 steps_elapsed: 47.632684\n",
      "loss: 8.150013, acc: 0.000000, lr: 0.001000, multi_batch_loss: 8.416186, multi_batch_acc: 0.007857, multi_batch_f1: 0.002444\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 98, in step\n",
      "    p.data.add_(perturb, alpha=-step_size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=56 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b5 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=45 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb5 \\\n",
    "--log_dir=work_landmark/log_efb5 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습7\n",
    "- model: efficientnet-b6\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, horizontal flip\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb6\n",
      "log_dir work_landmark/log_efb6\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name efficientnet-b6\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 25\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 32\n",
      "val_batch_size 64\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/25, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/25,step:49/2479,2020/11/21 02:26:37] total_elapsed: 0:00:40.533286, batch_elapsed: 0.758253, 50 steps_elapsed: 39.366781\n",
      "loss: 6.942018, acc: 0.000000, lr: 0.001000, multi_batch_loss: 6.940813, multi_batch_acc: 0.006875, multi_batch_f1: 0.001896\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 248, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=32 \\\n",
    "--val_batch_size=64 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=efficientnet-b6 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=25 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb6 \\\n",
    "--log_dir=work_landmark/log_efb6 \\\n",
    "--transform_func_name=get_train_transforms_simple_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습8\n",
    "- model: efficientnet-b7\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, horizontal flip\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb7\n",
      "log_dir work_landmark/log_efb7\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name efficientnet-b7\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 29\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 32\n",
      "val_batch_size 64\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/29, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/29,step:49/2479,2020/11/21 02:29:56] total_elapsed: 0:00:53.134736, batch_elapsed: 0.994612, 50 steps_elapsed: 52.009395\n",
      "loss: 6.915792, acc: 0.000000, lr: 0.001000, multi_batch_loss: 6.926434, multi_batch_acc: 0.009375, multi_batch_f1: 0.002216\n",
      "[train-epoch:01/29,step:99/2479,2020/11/21 02:30:48] total_elapsed: 0:01:45.223603, batch_elapsed: 0.991487, 50 steps_elapsed: 50.900134\n",
      "loss: 6.473351, acc: 0.031250, lr: 0.001000, multi_batch_loss: 6.752207, multi_batch_acc: 0.023125, multi_batch_f1: 0.006739\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=32 \\\n",
    "--val_batch_size=64 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=efficientnet-b7 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=29 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_efb7 \\\n",
    "--log_dir=work_landmark/log_efb7 \\\n",
    "--transform_func_name=get_train_transforms_simple_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습9\n",
    "- model: fishnet150\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 224(h)x224(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_fishnet150\n",
      "log_dir work_landmark/log_fishnet150\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name fishnet150\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 31\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 56\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 224\n",
      "pretrained False\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/31, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/31,step:49/1417,2020/11/21 02:34:45] total_elapsed: 0:00:26.129005, batch_elapsed: 0.465784, 50 steps_elapsed: 24.831044\n",
      "loss: 6.949007, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.231360, multi_batch_acc: 0.001071, multi_batch_f1: 0.000039\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 39, in _projection\n",
      "    if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=56 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=fishnet150 \\\n",
    "--input_size=224 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=31 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_fishnet150 \\\n",
    "--log_dir=work_landmark/log_fishnet150 \\\n",
    "--transform_func_name=get_train_transforms_simple_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습10\n",
    "- model: fishnet150\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 224(h)x224(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_fishnet201\n",
      "log_dir work_landmark/log_fishnet201\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name fishnet201\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 24\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 56\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 224\n",
      "pretrained False\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/24, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/24,step:49/1417,2020/11/21 02:35:50] total_elapsed: 0:00:34.930310, batch_elapsed: 0.640640, 50 steps_elapsed: 33.408738\n",
      "loss: 7.131888, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.206005, multi_batch_acc: 0.001429, multi_batch_f1: 0.000052\n",
      "[train-epoch:01/24,step:99/1417,2020/11/21 02:36:24] total_elapsed: 0:01:09.447111, batch_elapsed: 0.641977, 50 steps_elapsed: 33.232589\n",
      "loss: 7.004349, acc: 0.000000, lr: 0.001000, multi_batch_loss: 6.979247, multi_batch_acc: 0.001786, multi_batch_f1: 0.000068\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "Traceback (most recent call last):\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "Traceback (most recent call last):\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 248, in train_epoch\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "    loss.backward()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/tensor.py\", line 198, in backward\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=56 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=fishnet201 \\\n",
    "--input_size=224 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=24 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_fishnet201 \\\n",
    "--log_dir=work_landmark/log_fishnet201 \\\n",
    "--transform_func_name=get_train_transforms_simple_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습11\n",
    "- model: resnext101_32x8d\n",
    "- augmentations: flip\n",
    "- input size: 216(h)x384(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_resnext\n",
      "log_dir work_landmark/log_resnext\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name resnext101_32x8d\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 33\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 64\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing False\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.2\n",
      "cutmix_prob 0.0\n",
      "beta 0.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /home/ubuntu/.cache/torch/checkpoints/resnext101_32x8d-8ba56ff5.pth\n",
      "100.0%\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/33, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/33,step:49/1240,2020/11/21 02:38:12] total_elapsed: 0:01:17.564229, batch_elapsed: 1.481887, 50 steps_elapsed: 75.740215\n",
      "loss: 7.000587, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.199999, multi_batch_acc: 0.001250, multi_batch_f1: 0.000050\n",
      "[train-epoch:01/33,step:99/1240,2020/11/21 02:39:29] total_elapsed: 0:02:34.824132, batch_elapsed: 1.448981, 50 steps_elapsed: 74.867432\n",
      "loss: 7.012872, acc: 0.000000, lr: 0.001000, multi_batch_loss: 6.997843, multi_batch_acc: 0.000313, multi_batch_f1: 0.000003\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 39, in _projection\n",
      "    if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=64 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=resnext101_32x8d \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=17 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_resnext \\\n",
    "--log_dir=work_landmark/log_resnext \\\n",
    "--transform_func_name=get_train_transforms_simple \\\n",
    "--pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습12\n",
    "- model: efficientnet-b7\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 356(h)x632(w)\n",
    "- trained with all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb7_alldata\n",
      "log_dir work_landmark/log_efb7_alldata\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name efficientnet-b7\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 21\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 4\n",
      "val_batch_size 8\n",
      "num_workers 8\n",
      "input_size 356,632\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train True\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/21, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/21,step:49/22026,2020/11/21 12:02:06] total_elapsed: 0:00:33.553461, batch_elapsed: 0.552296, 50 steps_elapsed: 33.011890\n",
      "loss: 6.900692, acc: 0.000000, lr: 0.001000, multi_batch_loss: 6.968808, multi_batch_acc: 0.005000, multi_batch_f1: 0.000203\n",
      "[train-epoch:01/21,step:99/22026,2020/11/21 12:02:35] total_elapsed: 0:01:01.787584, batch_elapsed: 0.545937, 50 steps_elapsed: 27.719396\n",
      "loss: 7.213740, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.017411, multi_batch_acc: 0.005000, multi_batch_f1: 0.000306\n",
      "[train-epoch:01/21,step:149/22026,2020/11/21 12:03:03] total_elapsed: 0:01:30.065155, batch_elapsed: 0.552129, 50 steps_elapsed: 27.800209\n",
      "loss: 7.203190, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.025565, multi_batch_acc: 0.000000, multi_batch_f1: 0.000000\n",
      "[train-epoch:01/21,step:199/22026,2020/11/21 12:03:31] total_elapsed: 0:01:58.308531, batch_elapsed: 0.543722, 50 steps_elapsed: 27.770231\n",
      "loss: 7.018430, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.022339, multi_batch_acc: 0.005000, multi_batch_f1: 0.000401\n",
      "[train-epoch:01/21,step:249/22026,2020/11/21 12:03:59] total_elapsed: 0:02:26.362405, batch_elapsed: 0.546648, 50 steps_elapsed: 27.604510\n",
      "loss: 7.311057, acc: 0.000000, lr: 0.001000, multi_batch_loss: 7.007092, multi_batch_acc: 0.005000, multi_batch_f1: 0.000383\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 248, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=4 \\\n",
    "--val_batch_size=8 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=efficientnet-b7 \\\n",
    "--input_size=356,632 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=21 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_alldata_efb7 \\\n",
    "--log_dir=work_landmark/log_alldata_efb7 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pretrained \\\n",
    "--use_all_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습13\n",
    "- model: efficientnet-b3\n",
    "- arcface loss\n",
    "- multiple pooling concat(GeM,MAC,SPoC)\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)\n",
    "- trained with all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb3_alldata\n",
      "log_dir work_landmark/log_efb3_alldata\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b3\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 41\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 52\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train True\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/41, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/41,step:49/1695,2020/11/21 12:05:13] total_elapsed: 0:00:28.288087, batch_elapsed: 0.503915, 50 steps_elapsed: 26.611162\n",
      "loss: 8.227262, acc: 0.000000, lr: 0.001000, multi_batch_loss: 8.375098, multi_batch_acc: 0.011923, multi_batch_f1: 0.002974\n",
      "[train-epoch:01/41,step:99/1695,2020/11/21 12:05:42] total_elapsed: 0:00:57.330240, batch_elapsed: 0.499959, 50 steps_elapsed: 26.818913\n",
      "loss: 7.536232, acc: 0.076923, lr: 0.001000, multi_batch_loss: 7.912804, multi_batch_acc: 0.033077, multi_batch_f1: 0.011171\n",
      "[train-epoch:01/41,step:149/1695,2020/11/21 12:06:11] total_elapsed: 0:01:26.281242, batch_elapsed: 0.500823, 50 steps_elapsed: 26.800088\n",
      "loss: 7.348120, acc: 0.096154, lr: 0.001000, multi_batch_loss: 7.399048, multi_batch_acc: 0.076154, multi_batch_f1: 0.030692\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 39, in _projection\n",
      "    if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=52 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b3 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=30 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_alldata_efb3_arcface \\\n",
    "--log_dir=work_landmark/log_alldata_efb3_arcface \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--pretrained \\\n",
    "--use_all_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습14\n",
    "- model: efficientnet-b3\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)\n",
    "- trained with all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb3_alldata\n",
      "log_dir work_landmark/log_efb3_alldata\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name efficientnet-b3\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 41\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 52\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train True\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/41, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/41,step:49/1695,2020/11/21 12:07:52] total_elapsed: 0:00:27.283371, batch_elapsed: 0.486350, 50 steps_elapsed: 25.650998\n",
      "loss: 6.834712, acc: 0.019231, lr: 0.001000, multi_batch_loss: 6.936358, multi_batch_acc: 0.014231, multi_batch_f1: 0.005339\n",
      "[train-epoch:01/41,step:99/1695,2020/11/21 12:08:21] total_elapsed: 0:00:55.686671, batch_elapsed: 0.486289, 50 steps_elapsed: 26.297261\n",
      "loss: 6.010252, acc: 0.057692, lr: 0.001000, multi_batch_loss: 6.492723, multi_batch_acc: 0.040385, multi_batch_f1: 0.014617\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 39, in _projection\n",
      "    if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=52 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=efficientnet-b3 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=42 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_alldata_efb3 \\\n",
    "--log_dir=work_landmark/log_alldata_efb3 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pretrained \\\n",
    "--use_all_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습15\n",
    "- model: efficientnet-b4\n",
    "- cutmix 0.5 probabiliy, beta 1.0\n",
    "- label smoothing: 0.1\n",
    "- augmentations: random crop, brightness, contrast, flip, shift, scale\n",
    "- input size: 216(h)x384(w)\n",
    "- trained with all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "save_dir work_landmark/cp_efb4_alldata\n",
      "log_dir work_landmark/log_efb4_alldata\n",
      "checkpoint_path None\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GAP\n",
      "model_name efficientnet-b4\n",
      "optimizer adamp\n",
      "scheduler step\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 41\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 52\n",
      "val_batch_size 128\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained True\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train True\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train True\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory True\n",
      "val_pin_memory True\n",
      "label_smoothing True\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.1\n",
      "cutmix_prob 0.5\n",
      "beta 1.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms_simple_bright_randomcrop\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "val_items 8778\n",
      "started to create train data loader\n",
      "created train data loader\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "Epoch 1/41, LR: 0.001000\n",
      "----------\n",
      "[train-epoch:01/41,step:49/1695,2020/11/21 12:10:45] total_elapsed: 0:00:35.087599, batch_elapsed: 0.639074, 50 steps_elapsed: 33.682516\n",
      "loss: 6.915558, acc: 0.038462, lr: 0.001000, multi_batch_loss: 6.945094, multi_batch_acc: 0.010769, multi_batch_f1: 0.002605\n",
      "[train-epoch:01/41,step:99/1695,2020/11/21 12:11:21] total_elapsed: 0:01:11.006983, batch_elapsed: 0.637254, 50 steps_elapsed: 33.776505\n",
      "loss: 6.351300, acc: 0.019231, lr: 0.001000, multi_batch_loss: 6.579141, multi_batch_acc: 0.039615, multi_batch_f1: 0.014415\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 509, in <module>\n",
      "    augmix_prob=args.augmix_prob, no_jsd=args.no_jsd, model_name=args.model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 52, in train_or_val\n",
      "    augmix_prob=augmix_prob, no_jsd=no_jsd, model_name=model_name)\n",
      "  File \"/home/ubuntu/source/dacon_landmark_classification/trainer.py\", line 249, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 91, in step\n",
      "    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/adamp/adamp.py\", line 39, in _projection\n",
      "    if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--optimizer=adamp \\\n",
    "--seed=1 \\\n",
    "--train \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--train_batch_size=52 \\\n",
    "--val_batch_size=128 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=efficientnet-b4 \\\n",
    "--input_size=216,384 \\\n",
    "--scheduler=step \\\n",
    "--lr_restart_step=1 \\\n",
    "--train_pin_memory \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_epochs=16 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--save_dir=work_landmark/cp_alldata_efb4 \\\n",
    "--log_dir=work_landmark/log_alldata_efb4 \\\n",
    "--transform_func_name=get_train_transforms_simple_bright_randomcrop \\\n",
    "--cutmix_prob=0.5 \\\n",
    "--beta=1.0 \\\n",
    "--label_smoothing \\\n",
    "--smoothing=0.1 \\\n",
    "--pretrained \\\n",
    "--use_all_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signle Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-21 13:00:35--  https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
      "Resolving font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)... 52.219.101.186\n",
      "Connecting to font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)|52.219.101.186|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 179644406 (171M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘efb3_arcface_alldata_e30.pth.8’\n",
      "\n",
      "efb3_arcface_alldat 100%[===================>] 171.32M  16.9MB/s    in 11s     \n",
      "\n",
      "2020-11-21 13:00:48 (15.0 MB/s) - ‘efb3_arcface_alldata_e30.pth.8’ saved [179644406/179644406]\n",
      "\n",
      "training params\n",
      "save_dir None\n",
      "log_dir ./log\n",
      "checkpoint_path ./efb3_arcface_alldata_e30.pth\n",
      "train_dir ./data/public/train/\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "pooling GeM,MAC,SPoC\n",
      "model_name arc_face,efficientnet-b3\n",
      "optimizer sgd\n",
      "scheduler cosine\n",
      "load_lr False\n",
      "lr_restart_step 1\n",
      "num_epochs 100\n",
      "log_step_interval 50\n",
      "num_classes 1049\n",
      "train_batch_size 64\n",
      "val_batch_size 256\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "pretrained False\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "lr 0.001\n",
      "lr_decay_gamma 0.9\n",
      "weight_decay 1e-05\n",
      "seed 1\n",
      "save_confusion_matrix False\n",
      "train False\n",
      "val True\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_all_train False\n",
      "train_val_data False\n",
      "not_val_shuffle False\n",
      "data_parallel False\n",
      "use_concat_pool False\n",
      "use_no_aug False\n",
      "use_no_color_aug False\n",
      "train_pin_memory False\n",
      "val_pin_memory True\n",
      "label_smoothing False\n",
      "use_benchmark True\n",
      "nesterov False\n",
      "smoothing 0.2\n",
      "cutmix_prob 0.0\n",
      "beta 0.0\n",
      "mixup_prob 0.0\n",
      "alpha 0.0\n",
      "augmix_prob 0.0\n",
      "center_crop_ratio 0.9\n",
      "no_jsd False\n",
      "use_gray False\n",
      "class_names normal,warning,chronic,deep\n",
      "transform_func_name get_train_transforms\n",
      "mixture_width 3\n",
      "mixture_depth -1\n",
      "aug_severity 1\n",
      "aug_prob_coeff 1.0\n",
      "all_ops False\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "Loading checkpoint './efb3_arcface_alldata_e30.pth'\n",
      "Loaded checkpoint './efb3_arcface_alldata_e30.pth' from iteration 52890\n",
      "val_items 8778\n",
      "created val data loader\n",
      "Start training or validating!\n",
      "started to validate val val dataset. samples: 8778\n",
      "[result_val_val-epoch:1,2020/11/21 13:01:20] total_elapsed: 0:00:23.851142, epoch_elapsed: 18.171289682388306, loss: 0.116574, acc: 1.000000, f1: 1.000000\n",
      "The end of validation.\n",
      "Training or Validation completed in 0m 24s\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
    "    \n",
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python main.py \\\n",
    "--checkpoint_path=./efb3_arcface_alldata_e30.pth \\\n",
    "--train_dir=./data/public/train/ \\\n",
    "--seed=1 \\\n",
    "--val \\\n",
    "--use_benchmark \\\n",
    "--val_batch_size=256 \\\n",
    "--log_step_interval=50 \\\n",
    "--model_name=arc_face,efficientnet-b3 \\\n",
    "--input_size=216,384 \\\n",
    "--val_pin_memory \\\n",
    "--num_classes=1049 \\\n",
    "--num_workers=8 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--pooling=GeM,MAC,SPoC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model Submission\n",
    "- efficientnet-b3\n",
    "- trained with all dataset\n",
    "- arcface\n",
    "- pooling concat (GeM,MAC,SPoC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-21 12:48:47--  https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
      "Resolving font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)... 52.219.104.112\n",
      "Connecting to font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)|52.219.104.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 179644406 (171M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘efb3_arcface_alldata_e30.pth.5’\n",
      "\n",
      "efb3_arcface_alldat 100%[===================>] 171.32M  17.4MB/s    in 11s     \n",
      "\n",
      "2020-11-21 12:48:59 (15.4 MB/s) - ‘efb3_arcface_alldata_e30.pth.5’ saved [179644406/179644406]\n",
      "\n",
      "training params\n",
      "output_csv_path work_landmark/submission_single_model.csv\n",
      "checkpoint_path ./efb3_arcface_alldata_e30.pth\n",
      "test_dir ./data/public/test/\n",
      "model_name arc_face,efficientnet-b3\n",
      "log_step_interval 10\n",
      "pooling GeM,MAC,SPoC\n",
      "num_classes 1049\n",
      "batch_size 256\n",
      "num_workers 8\n",
      "input_size 216,384\n",
      "is_different_class_num False\n",
      "not_dict_model False\n",
      "seed 1\n",
      "use_crop False\n",
      "use_center_crop False\n",
      "use_pad False\n",
      "use_benchmark True\n",
      "center_crop_ratio 0.9\n",
      "use_gray False\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "Loading checkpoint './efb3_arcface_alldata_e30.pth'\n",
      "Loaded checkpoint './efb3_arcface_alldata_e30.pth' from iteration 52890\n",
      "10 149\n",
      "20 149\n",
      "30 149\n",
      "40 149\n",
      "50 149\n",
      "60 149\n",
      "70 149\n",
      "80 149\n",
      "90 149\n",
      "100 149\n",
      "110 149\n",
      "120 149\n",
      "130 149\n",
      "140 149\n",
      "done\n",
      "submission top 3 lines.\n",
      "id,landmark_id,conf\n",
      "ze8yn3ryox,207,0.83569986\n",
      "zmfqzcv3jg,962,0.9492601\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python submit.py \\\n",
    "--output_csv_path=work_landmark/submission_single_model.csv \\\n",
    "--checkpoint_path=./efb3_arcface_alldata_e30.pth \\\n",
    "--test_dir=./data/public/test/ \\\n",
    "--model_name=arc_face,efficientnet-b3 \\\n",
    "--num_workers=8 \\\n",
    "--num_classes=1049 \\\n",
    "--batch_size=256 \\\n",
    "--input_size=216,384 \\\n",
    "--seed=1 \\\n",
    "--pooling=GeM,MAC,SPoC \\\n",
    "--use_benchmark\n",
    "\n",
    "!echo \"submission top 3 lines.\"\n",
    "\n",
    "!head -n 3 work_landmark/submission_single_model.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Validation\n",
    "efficientnet-b3, b4 두 모델 ensemble 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-21 13:09:22--  https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
      "Resolving font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)... 52.219.96.32\n",
      "Connecting to font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)|52.219.96.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 179644406 (171M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘efb3_arcface_alldata_e30.pth.10’\n",
      "\n",
      "efb3_arcface_alldat 100%[===================>] 171.32M  17.3MB/s    in 11s     \n",
      "\n",
      "2020-11-21 13:09:34 (15.3 MB/s) - ‘efb3_arcface_alldata_e30.pth.10’ saved [179644406/179644406]\n",
      "\n",
      "--2020-11-21 13:09:34--  https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb4_arcface_epoch_32.pth\n",
      "Resolving font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)... 52.219.80.128\n",
      "Connecting to font-recognizer-bucket.s3.us-east-2.amazonaws.com (font-recognizer-bucket.s3.us-east-2.amazonaws.com)|52.219.80.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 267845705 (255M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘efb4_arcface_epoch_32.pth.1’\n",
      "\n",
      "efb4_arcface_epoch_ 100%[===================>] 255.44M  16.1MB/s    in 17s     \n",
      "\n",
      "2020-11-21 13:09:52 (14.8 MB/s) - ‘efb4_arcface_epoch_32.pth.1’ saved [267845705/267845705]\n",
      "\n",
      "training params\n",
      "csv None\n",
      "pkl_dir None\n",
      "output_dir None\n",
      "checkpoint_paths ./efb3_arcface_alldata_e30.pth,efb4_arcface_epoch_32.pth\n",
      "use_glob False\n",
      "image_dir ./data/public/train\n",
      "val_ratio 0.1\n",
      "label_file data/public/train.csv\n",
      "test_dir None\n",
      "weights 0.6,0.4\n",
      "model_names arc_face,efficientnet-b3:arc_face,efficientnet-b4\n",
      "log_step_interval 10\n",
      "poolings GeM,MAC,SPoC:GeM,MAC,SPoC\n",
      "num_classes 1049\n",
      "batch_size 256\n",
      "num_workers 8\n",
      "input_sizes 216x384,216x384\n",
      "eval True\n",
      "save False\n",
      "test False\n",
      "from_pkl False\n",
      "use_pad False\n",
      "seed 1\n",
      "use_crops None\n",
      "use_center_crop False\n",
      "center_crop_ratio 0.9\n",
      "use_gray None\n",
      "val samples 8778\n",
      "./efb3_arcface_alldata_e30.pth 0 2\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "Loading checkpoint './efb3_arcface_alldata_e30.pth'\n",
      "Loaded checkpoint './efb3_arcface_alldata_e30.pth' from iteration 52890\n",
      "input size (216, 384)\n",
      "evaluating 10 35\n",
      "evaluating 20 35\n",
      "evaluating 30 35\n",
      "acc: 1.0, f1: 1.0\n",
      "efb4_arcface_epoch_32.pth 1 2\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "Loading checkpoint 'efb4_arcface_epoch_32.pth'\n",
      "Loaded checkpoint 'efb4_arcface_epoch_32.pth' from iteration 39680\n",
      "input size (216, 384)\n",
      "evaluating 10 35\n",
      "evaluating 20 35\n",
      "evaluating 30 35\n",
      "acc: 0.9965823650034177, f1: 0.9966261610168383\n",
      "last acc 0.9996582365003418\n",
      "last fq 0.9996765627127876\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb4_arcface_epoch_32.pth\n",
    "    \n",
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python ensemble_submit.py \\\n",
    "--checkpoint_paths=./efb3_arcface_alldata_e30.pth,efb4_arcface_epoch_32.pth \\\n",
    "--model_names=arc_face,efficientnet-b3:arc_face,efficientnet-b4 \\\n",
    "--input_sizes=216x384,216x384 \\\n",
    "--poolings=GeM,MAC,SPoC:GeM,MAC,SPoC \\\n",
    "--weights=0.6,0.4 \\\n",
    "--label_file=data/public/train.csv \\\n",
    "--image_dir=./data/public/train \\\n",
    "--num_classes=1049 \\\n",
    "--batch_size=256 \\\n",
    "-w=8 \\\n",
    "--eval \\\n",
    "--seed=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘efb3_arcface_alldata_e30.pth’ already there; not retrieving.\n",
      "\n",
      "File ‘efb4_arcface_epoch_32.pth’ already there; not retrieving.\n",
      "\n",
      "training params\n",
      "csv ./work_landmark/submission/ensemble_submision.csv\n",
      "pkl_dir None\n",
      "output_dir None\n",
      "checkpoint_paths ./efb3_arcface_alldata_e30.pth,efb4_arcface_epoch_32.pth\n",
      "use_glob False\n",
      "image_dir None\n",
      "val_ratio 0.1\n",
      "label_file None\n",
      "test_dir ./data/public/test\n",
      "weights 0.6,0.4\n",
      "model_names arc_face,efficientnet-b3:arc_face,efficientnet-b4\n",
      "log_step_interval 10\n",
      "poolings GeM,MAC,SPoC:GeM,MAC,SPoC\n",
      "num_classes 1049\n",
      "batch_size 256\n",
      "num_workers 8\n",
      "input_sizes 216x384,216x384\n",
      "eval False\n",
      "save False\n",
      "test True\n",
      "from_pkl False\n",
      "use_pad False\n",
      "seed 1\n",
      "use_crops None\n",
      "use_center_crop False\n",
      "center_crop_ratio 0.9\n",
      "use_gray None\n",
      "all checkpoints ['./efb3_arcface_alldata_e30.pth', 'efb4_arcface_epoch_32.pth']\n",
      "./efb3_arcface_alldata_e30.pth 0 2\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "Loading checkpoint './efb3_arcface_alldata_e30.pth'\n",
      "Loaded checkpoint './efb3_arcface_alldata_e30.pth' from iteration 52890\n",
      "input size (216, 384)\n",
      "testing 10 149\n",
      "testing 20 149\n",
      "testing 30 149\n",
      "testing 40 149\n",
      "testing 50 149\n",
      "testing 60 149\n",
      "testing 70 149\n",
      "testing 80 149\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "\n",
      "head 3 lines of submission file\n",
      "id,landmark_id,conf\n",
      "000b85kx5d,846,0.9156126\n",
      "002msp5aqm,7,0.8492743\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb4_arcface_epoch_32.pth\n",
    "    \n",
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python ensemble_submit.py \\\n",
    "--checkpoint_paths=./efb3_arcface_alldata_e30.pth,efb4_arcface_epoch_32.pth \\\n",
    "--model_names=arc_face,efficientnet-b3:arc_face,efficientnet-b4 \\\n",
    "--input_sizes=216x384,216x384 \\\n",
    "--poolings=GeM,MAC,SPoC:GeM,MAC,SPoC \\\n",
    "--weights=0.6,0.4 \\\n",
    "--test_dir=./data/public/test \\\n",
    "--num_classes=1049 \\\n",
    "--batch_size=256 \\\n",
    "-w=8 \\\n",
    "--test \\\n",
    "--csv=./work_landmark/submission/ensemble_submision.csv \\\n",
    "--seed=1\n",
    "\n",
    "!echo \"\"\n",
    "!echo \"head 3 lines of submission file\"\n",
    "!head -n 3 ./work_landmark/submission/ensemble_submision.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Submission (15models ensemble submission)\n",
    "- 위에 15가지 모델이 모두 학습된 모델 파일이 정확한 경로에 저장되어 있어야만 동작합니다.\n",
    "\n",
    "- 앙상블할 모델 순서를 정렬하면 아래 순서로 정렬되기 때문에 정렬된 순서데로 모델의 가중치나 하이퍼 파라미터들을 잘 세팅해야 합니다.\n",
    "- 모델 순서: 4 efficient models(trained with all dataset, efb3, efb3_arc_face, efb4, efb7), 8 efficientnet models(b0 to b8), 2 fishnet models, 1 resnext model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘efb3_arcface_alldata_e30.pth’ already there; not retrieving.\n",
      "\n",
      "cp_efb0 copied\n",
      "cp_efb1 copied\n",
      "cp_efb2 copied\n",
      "cp_efb3 copied\n",
      "cp_efb4 copied\n",
      "cp_efb5 copied\n",
      "cp_efb6 copied\n",
      "cp_efb7 copied\n",
      "cp_alldata_efb3 copied\n",
      "cp_alldata_efb3_arcface copied\n",
      "cp_alldata_efb4 copied\n",
      "cp_alldata_efb7 copied\n",
      "cp_fishnet150 copied\n",
      "cp_fishnet201 copied\n",
      "cp_resnext copied\n"
     ]
    }
   ],
   "source": [
    "# 가상으로 동작하게 하도록 하기 위해 한 모델을 다운로드 하여 모든 디렉토리에 저장하고 실행되도록  하는 코드입니다.\n",
    "!wget -nc https://font-recognizer-bucket.s3.us-east-2.amazonaws.com/resource/kaggle/efb3_arcface_alldata_e30.pth\n",
    "import shutil\n",
    "import os\n",
    "path_list  = ['cp_efb0','cp_efb1','cp_efb2','cp_efb3','cp_efb4','cp_efb5','cp_efb6','cp_efb7','cp_alldata_efb3','cp_alldata_efb3_arcface','cp_alldata_efb4','cp_alldata_efb7','cp_fishnet150','cp_fishnet201','cp_resnext']\n",
    "model_path = \"efb3_arcface_alldata_e30.pth\"\n",
    "\n",
    "for path in path_list:\n",
    "    os.makedirs(os.path.join('work_landmark', path), exist_ok=True)\n",
    "    shutil.copy(model_path, os.path.join('work_landmark', path, \"val_best.pth\"))\n",
    "    print(path, \"copied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training params\n",
      "csv ./work_landmark/submission/15models_ensemble_submision.csv\n",
      "pkl_dir None\n",
      "output_dir None\n",
      "checkpoint_paths ./work_landmark/cp_*/val_best.pth\n",
      "use_glob True\n",
      "image_dir None\n",
      "val_ratio 0.1\n",
      "label_file None\n",
      "test_dir ./data/public/test\n",
      "weights 0.2,0.2,0.2,0.2,0.1,0.1,0.1,0.5,0.2,0.1,0.1,0.6,0.08,0.08,0.05\n",
      "model_names efficientnet-b3:arc_face,efficientnet-b3:efficientnet-b4:efficientnet-b7:arc_face,efficientnet-b0:efficientnet-b1:arc_face,efficientnet-b2:arc_face,efficientnet-b3:arc_fcae,efficientnet-b4:arc_face,efficientnet-b5:efficientnet-b6:efficientnet-b7:fishnet150:fishnet201:resnext101_32x8d\n",
      "log_step_interval 10\n",
      "poolings GAP:GeM,MAC,SPoC:GAP:GAP:GeM,MAC,SPoC:GAP:GeM,MAC,SPoC:GeM,MAC,SPoC:GeM,MAC,SPoC:GeM,MAC,SPoC:GAP:GAP:GAP:GAP:GAP\n",
      "num_classes 1049\n",
      "batch_size 256\n",
      "num_workers 8\n",
      "input_sizes 216x384,216x384,216x384,356x632,216x384,216x384,216x384,216x384,216x384,216x384,216x384,216x384,224x224,224x224,216x384\n",
      "eval False\n",
      "save False\n",
      "test True\n",
      "from_pkl False\n",
      "use_pad False\n",
      "not_strict True\n",
      "seed 1\n",
      "use_crops None\n",
      "use_center_crop False\n",
      "center_crop_ratio 0.9\n",
      "use_gray None\n",
      "all checkpoints ['./work_landmark/cp_alldata_efb3/val_best.pth', './work_landmark/cp_alldata_efb3_arcface/val_best.pth', './work_landmark/cp_alldata_efb4/val_best.pth', './work_landmark/cp_alldata_efb7/val_best.pth', './work_landmark/cp_efb0/val_best.pth', './work_landmark/cp_efb1/val_best.pth', './work_landmark/cp_efb2/val_best.pth', './work_landmark/cp_efb3/val_best.pth', './work_landmark/cp_efb4/val_best.pth', './work_landmark/cp_efb5/val_best.pth', './work_landmark/cp_efb6/val_best.pth', './work_landmark/cp_efb7/val_best.pth', './work_landmark/cp_fishnet150/val_best.pth', './work_landmark/cp_fishnet201/val_best.pth', './work_landmark/cp_resnext/val_best.pth']\n",
      "./work_landmark/cp_alldata_efb3/val_best.pth 0 15\n",
      "Loading checkpoint './work_landmark/cp_alldata_efb3/val_best.pth'\n",
      "Loaded checkpoint './work_landmark/cp_alldata_efb3/val_best.pth' from iteration 52890\n",
      "input size (216, 384)\n",
      "testing 10 149\n",
      "testing 20 149\n",
      "testing 30 149\n",
      "testing 40 149\n",
      "testing 50 149\n",
      "testing 60 149\n",
      "testing 70 149\n",
      "testing 80 149\n",
      "testing 90 149\n",
      "testing 100 149\n",
      "testing 110 149\n",
      "testing 120 149\n",
      "testing 130 149\n",
      "testing 140 149\n",
      "./work_landmark/cp_alldata_efb3_arcface/val_best.pth 1 15\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "Loading checkpoint './work_landmark/cp_alldata_efb3_arcface/val_best.pth'\n",
      "Loaded checkpoint './work_landmark/cp_alldata_efb3_arcface/val_best.pth' from iteration 52890\n",
      "input size (216, 384)\n",
      "testing 10 149\n",
      "testing 20 149\n",
      "testing 30 149\n",
      "testing 40 149\n",
      "testing 50 149\n",
      "testing 60 149\n",
      "testing 70 149\n",
      "testing 80 149\n",
      "testing 90 149\n",
      "testing 100 149\n",
      "testing 110 149\n",
      "testing 120 149\n",
      "testing 130 149\n",
      "testing 140 149\n",
      "./work_landmark/cp_alldata_efb4/val_best.pth 2 15\n",
      "Loading checkpoint './work_landmark/cp_alldata_efb4/val_best.pth'\n",
      "Loaded checkpoint './work_landmark/cp_alldata_efb4/val_best.pth' from iteration 52890\n",
      "input size (216, 384)\n",
      "testing 10 149\n",
      "testing 20 149\n",
      "testing 30 149\n",
      "testing 40 149\n",
      "testing 50 149\n",
      "testing 60 149\n",
      "testing 70 149\n",
      "testing 80 149\n",
      "testing 90 149\n",
      "testing 100 149\n",
      "testing 110 149\n",
      "testing 120 149\n",
      "testing 130 149\n",
      "testing 140 149\n",
      "./work_landmark/cp_alldata_efb7/val_best.pth 3 15\n",
      "Loading checkpoint './work_landmark/cp_alldata_efb7/val_best.pth'\n",
      "Loaded checkpoint './work_landmark/cp_alldata_efb7/val_best.pth' from iteration 52890\n",
      "input size (356, 632)\n",
      "Traceback (most recent call last):\n",
      "  File \"ensemble_submit.py\", line 408, in <module>\n",
      "    main(args)\n",
      "  File \"ensemble_submit.py\", line 260, in main\n",
      "    outputs = model(imgs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/efficientnet_pytorch/model.py\", line 193, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/efficientnet_pytorch/model.py\", line 182, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/efficientnet_pytorch/model.py\", line 77, in forward\n",
      "    x = self._swish(self._bn0(self._expand_conv(inputs)))\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\", line 106, in forward\n",
      "    exponential_average_factor, self.eps)\n",
      "  File \"/home/ubuntu/anaconda3/envs/dacon_landmark/lib/python3.6/site-packages/torch/nn/functional.py\", line 1923, in batch_norm\n",
      "    training, momentum, eps, torch.backends.cudnn.enabled\n",
      "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n",
      "head: cannot open './work_landmark/submission/15models_ensemble_submision.csv' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 정상적인 결과를 보기위해서는 15개 모델을 모두 학습후에 진행해야 합니다.\n",
    "# 정확한 확인을 위해서는 --not_strict 인자를 제거해야 합니다.(가상으로 동작하게 하기 위한 인자)\n",
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python ensemble_submit.py \\\n",
    "--checkpoint_paths=./work_landmark/cp_*/val_best.pth \\\n",
    "--model_names=efficientnet-b3:arc_face,efficientnet-b3:efficientnet-b4:efficientnet-b7:arc_face,efficientnet-b0:efficientnet-b1:arc_face,efficientnet-b2:arc_face,efficientnet-b3:arc_fcae,efficientnet-b4:arc_face,efficientnet-b5:efficientnet-b6:efficientnet-b7:fishnet150:fishnet201:resnext101_32x8d \\\n",
    "--input_sizes=216x384,216x384,216x384,356x632,216x384,216x384,216x384,216x384,216x384,216x384,216x384,216x384,224x224,224x224,216x384 \\\n",
    "--poolings=GAP:GeM,MAC,SPoC:GAP:GAP:GeM,MAC,SPoC:GAP:GeM,MAC,SPoC:GeM,MAC,SPoC:GeM,MAC,SPoC:GeM,MAC,SPoC:GAP:GAP:GAP:GAP:GAP \\\n",
    "--weights=0.2,0.2,0.2,0.2,0.1,0.1,0.1,0.5,0.2,0.1,0.1,0.6,0.08,0.08,0.05 \\\n",
    "--test_dir=./data/public/test \\\n",
    "--num_classes=1049 \\\n",
    "--batch_size=256 \\\n",
    "-w=8 \\\n",
    "--test \\\n",
    "--use_glob \\\n",
    "--csv=./work_landmark/submission/15models_ensemble_submision.csv \\\n",
    "--seed=1 \\\n",
    "--not_strict\n",
    "                                                                                                                \n",
    "!head -n 3 ./work_landmark/submission/15models_ensemble_submision.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
